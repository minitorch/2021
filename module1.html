

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Module 1 - Auto-Differentiation &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="_static/thebelab.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Derivatives" href="derivative.html" />
    <link rel="prev" title="Visualization" href="visualization.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Module 1 - Auto-Differentiation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="derivative.html">Derivatives</a></li>
<li class="toctree-l2"><a class="reference internal" href="scalar.html">Tracking Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="chainrule.html">Autodifferentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="backpropagate.html">Backpropagation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tasks">Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#task-1-1-numerical-derivatives">Task 1.1: Numerical Derivatives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-1-2-scalars">Task 1.2: Scalars</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-1-3-chain-rule">Task 1.3: Chain Rule</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-1-4-backpropagation">Task 1.4: Backpropagation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-1-5-training">Task 1.5: Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module2.html">Module 2 - Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Module 4 - Networks</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Module 1 - Auto-Differentiation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/module1.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-1-auto-differentiation">
<h1>Module 1 - Auto-Differentiation<a class="headerlink" href="#module-1-auto-differentiation" title="Permalink to this headline">¶</a></h1>
<img alt="_images/backprop4.png" class="align-center" src="_images/backprop4.png" />
<p>This module shows how to build the first version of MiniTorch
(mini-MiniTorch?) using only Scalar values. This covers key aspects
of auto-differentiation: the key technique in the system. Then you will
use your code to train a preliminary model.</p>
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-1">https://github.com/minitorch/Module-1</a> .</p>
<p>To begin, remember to activate your virtual environment first, and then
clone your assignment:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT1_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT_NAME</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Ue</span> <span class="o">.</span>
</pre></div>
</div>
<p>Module 1 is built upon the previous Module 0, so make sure to pull your
files from Assignment 0
to your new repo.</p>
<p>Please continue to follow the <a class="reference internal" href="contributing.html"><span class="doc">Contributing</span></a> guideline.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="derivative.html">Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="scalar.html">Tracking Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chainrule.html">Autodifferentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagate.html">Backpropagation</a></li>
</ul>
</div>
<div class="section" id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="task-1-1-numerical-derivatives">
<h3>Task 1.1: Numerical Derivatives<a class="headerlink" href="#task-1-1-numerical-derivatives" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with derivatives.
Be sure to review <a class="reference external" href="https://en.wikipedia.org/wiki/Differentiation_rules">differentiation rules</a>
and the notation for derivatives.
Then carefully read the Guide on
<a class="reference internal" href="derivative.html"><span class="doc">Derivatives</span></a>.</p>
</div>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/scalar.py</cite> and pass tests
marked as <cite>task1_1</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.scalar.central_difference">
<code class="sig-prename descclassname">minitorch.scalar.</code><code class="sig-name descname">central_difference</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">vals</span></em>, <em class="sig-param"><span class="n">arg</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="o">=</span><span class="default_value">1e-06</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.central_difference" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes an approximation to the derivative of <cite>f</cite> with respect to one arg.</p>
<p>See <a class="reference internal" href="derivative.html"><span class="doc">Derivatives</span></a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Finite_difference">https://en.wikipedia.org/wiki/Finite_difference</a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> -- arbitrary function from n-scalar args to one value</p></li>
<li><p><strong>*vals</strong> (<em>list of floats</em>) -- n-float values <span class="math notranslate nohighlight">\(x_0 \ldots x_{n-1}\)</span></p></li>
<li><p><strong>arg</strong> (<em>int</em>) -- the number <span class="math notranslate nohighlight">\(i\)</span> of the arg to compute the derivative</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) -- a small constant</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An approximation of <span class="math notranslate nohighlight">\(f'_i(x_0, \ldots, x_{n-1})\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-1-2-scalars">
<h3>Task 1.2: Scalars<a class="headerlink" href="#task-1-2-scalars" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with the <a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a> class.
Be sure to first carefully read the Guide on
<a class="reference internal" href="scalar.html"><span class="doc">Tracking Variables</span></a> and to refresh your memory on <a class="reference external" href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types/">Python numerical overrides</a>.</p>
</div>
<p>Implement the overridden mathematical functions required for the
<a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a> class.
Each of these requires wiring the internal Python operator to the correct
<code class="xref py py-func docutils literal notranslate"><span class="pre">minitorch.Function.forward()</span></code> call.</p>
<dl class="py function">
<dt id="minitorch.scalar.ScalarFunction.forward">
<code class="sig-prename descclassname">minitorch.scalar.ScalarFunction.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ScalarFunction.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward call, compute <span class="math notranslate nohighlight">\(f(x_0 \ldots x_{n-1})\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Context</span></code>) -- A container object to save
any information that may be needed
for the call to backward.</p></li>
<li><p><strong>*inputs</strong> (<em>list of floats</em>) -- n-float values <span class="math notranslate nohighlight">\(x_0 \ldots x_{n-1}\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>Should return float the computation of the function <span class="math notranslate nohighlight">\(f\)</span>.</p>
</dd></dl>

<p>Read the example ScalarFunctions that we have implemented for guidelines. You
may find it useful to reuse the operators from Module 0.</p>
<p>We have built a debugging tool for you to observe the workings of your
expressions to see
how the graph is built. You can run it in <cite>project/show_expression.py</cite>. You
need to install
pydot and networkx. You can alter
the expression at the top of the file and then run the code to create a
graph in <cite>Visdom</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">expression</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">Scalar</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mf">10.</span>
    <span class="n">y</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;y&quot;</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">python</span> <span class="n">project</span><span class="o">/</span><span class="n">show_expression</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<img alt="_images/expgraph.png" class="align-center" src="_images/expgraph.png" />
<div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/scalar.py</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.scalar.Mul.forward">
<code class="sig-prename descclassname">minitorch.scalar.Mul.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Mul.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Inv.forward">
<code class="sig-prename descclassname">minitorch.scalar.Inv.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Inv.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Neg.forward">
<code class="sig-prename descclassname">minitorch.scalar.Neg.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Neg.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Sigmoid.forward">
<code class="sig-prename descclassname">minitorch.scalar.Sigmoid.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Sigmoid.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.ReLU.forward">
<code class="sig-prename descclassname">minitorch.scalar.ReLU.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Exp.forward">
<code class="sig-prename descclassname">minitorch.scalar.Exp.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Exp.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/scalar.py</cite>, and pass
tests marked as <cite>task1_2</cite>.
See <a class="reference external" href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types/">Python numerical overrides</a>
for the interface of these methods. All of these functions should return
<a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a> arguments.</p>
</div>
<dl class="py function">
<dt id="minitorch.Scalar.__lt__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__lt__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__lt__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__gt__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__gt__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__gt__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__sub__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__sub__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__sub__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__neg__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__neg__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__neg__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__add__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__add__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__add__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.log">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.log" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.exp">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">exp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.exp" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.sigmoid">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">sigmoid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.sigmoid" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.relu">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">relu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.relu" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="task-1-3-chain-rule">
<h3>Task 1.3: Chain Rule<a class="headerlink" href="#task-1-3-chain-rule" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task is quite tricky, so be sure you
understand the chain rule, Variables, and Functions.
Be sure to first read the Guide on
<a class="reference internal" href="chainrule.html"><span class="doc">Autodifferentiation</span></a> very carefully and read the code for other ScalarFunctions.</p>
</div>
<p>Implement the <cite>chain_rule</cite> function on FunctionBase for functions of arbitrary
arguments.
This function should be able to backward process a function by passing it in
a context and <span class="math notranslate nohighlight">\(d_{out}\)</span> and then collecting the local derivatives. It
should then pair these with the right variables and return them. This function
is also where we filter out constants that were used on the forward pass,
but do not need derivatives.</p>
<div class="admonition-todo admonition" id="id5">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/autodiff.py</cite>, and pass
tests marked as <cite>task1_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.FunctionBase.chain_rule">
<code class="sig-prename descclassname">minitorch.FunctionBase.</code><code class="sig-name descname">chain_rule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.FunctionBase.chain_rule" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement the derivative chain-rule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Context</span></code>) -- The context from running forward</p></li>
<li><p><strong>inputs</strong> (<em>list of args</em>) -- The args that were passed to <code class="xref py py-func docutils literal notranslate"><span class="pre">FunctionBase.apply()</span></code> (e.g. <span class="math notranslate nohighlight">\(x, y\)</span>)</p></li>
<li><p><strong>d_output</strong> (<em>number</em>) -- The <cite>d_output</cite> value in the chain rule.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of (<cite>Variable</cite>, number) A list of non-constant variables with their derivatives
(see <cite>is_constant</cite> to remove unneeded variables)</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-1-4-backpropagation">
<h3>Task 1.4: Backpropagation<a class="headerlink" href="#task-1-4-backpropagation" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Be sure to first read the Guide on
<a class="reference internal" href="backpropagate.html"><span class="doc">Backpropagation</span></a> very carefully and read the code for other
ScalarFunctions.</p>
</div>
<p>Implement backpropagation. Each of these requires wiring the internal Python
operator to the correct
<code class="xref py py-func docutils literal notranslate"><span class="pre">minitorch.Function.backward()</span></code> call.</p>
<dl class="py function">
<dt id="minitorch.scalar.ScalarFunction.backward">
<code class="sig-prename descclassname">minitorch.scalar.ScalarFunction.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_out</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ScalarFunction.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Backward call, computes <span class="math notranslate nohighlight">\(f'_{x_i}(x_0 \ldots x_{n-1}) \times d_{out}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>Context</em>) -- A container object holding any information saved during in the corresponding <cite>forward</cite> call.</p></li>
<li><p><strong>d_out</strong> (<em>float</em>) -- <span class="math notranslate nohighlight">\(d_out\)</span> term in the chain rule.</p></li>
</ul>
</dd>
</dl>
<p>Should return the computation of the derivative function
<span class="math notranslate nohighlight">\(f'_{x_i}\)</span> for each input <span class="math notranslate nohighlight">\(x_i\)</span> times <cite>d_out</cite>.</p>
</dd></dl>

<p>Read the example ScalarFunctions that we have implemented for
guidelines. Feel free to also consult <a class="reference external" href="https://en.wikipedia.org/wiki/Differentiation_rules">differentiation rules</a> if you forget how
these identities work.</p>
<div class="admonition-todo admonition" id="id7">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/autodiff.py</cite> and
<cite>minitorch/scalar.py</cite>,
and pass tests marked as <cite>task1_4</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.backpropagate">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">backpropagate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">variable</span></em>, <em class="sig-param"><span class="n">deriv</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.backpropagate" title="Permalink to this definition">¶</a></dt>
<dd><p>Runs a breadth-first search on the computation graph in order to
backpropagate derivatives to the leaves.</p>
<p>See <a class="reference internal" href="backpropagate.html"><span class="doc">Backpropagation</span></a> for details on the algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<a class="reference internal" href="scalar.html#minitorch.Variable" title="minitorch.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a>) -- The final variable</p></li>
<li><p><strong>deriv</strong> (<em>number</em>) -- Its derivative that we want to propagate backward to the leaves.</p></li>
</ul>
</dd>
</dl>
<p>No return. Should write to its results to the derivative values of each leaf.</p>
</dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Mul.backward">
<code class="sig-prename descclassname">minitorch.scalar.Mul.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Mul.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Inv.backward">
<code class="sig-prename descclassname">minitorch.scalar.Inv.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Inv.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Neg.backward">
<code class="sig-prename descclassname">minitorch.scalar.Neg.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Neg.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Sigmoid.backward">
<code class="sig-prename descclassname">minitorch.scalar.Sigmoid.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Sigmoid.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.ReLU.backward">
<code class="sig-prename descclassname">minitorch.scalar.ReLU.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ReLU.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Exp.backward">
<code class="sig-prename descclassname">minitorch.scalar.Exp.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Exp.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="task-1-5-training">
<h3>Task 1.5: Training<a class="headerlink" href="#task-1-5-training" title="Permalink to this headline">¶</a></h3>
<p>If your code works, you should now be able to run the training script.
Study the code in <cite>project/run_scalar.py</cite> carefully to understand what
the neural network is doing. You will also need your Module code from the
previous assignment as well. You can modify the dataset and the module with
the parameters at the top of the file. Start with this simple config:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PTS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">DATASET</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Simple</span><span class="p">(</span><span class="n">PTS</span><span class="p">,</span> <span class="n">vis</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">HIDDEN</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">RATE</span> <span class="o">=</span> <span class="mf">0.5</span>
</pre></div>
</div>
<img alt="_images/simple.png" src="_images/simple.png" />
<p>You can then move up to something more complex, for instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PTS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">DATASET</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">Xor</span><span class="p">(</span><span class="n">PTS</span><span class="p">,</span> <span class="n">vis</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">HIDDEN</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">RATE</span> <span class="o">=</span> <span class="mf">0.5</span>
</pre></div>
</div>
<p>If your code is successful, you should be able to fit the data like this:</p>
<img alt="_images/complete.png" src="_images/complete.png" />
<div class="admonition-todo admonition" id="id8">
<p class="admonition-title">Todo</p>
<p>Train a scalar model, and add the output training logs and final images
like the ones above to your README file.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="derivative.html" class="btn btn-neutral float-right" title="Derivatives" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="visualization.html" class="btn btn-neutral float-left" title="Visualization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Sasha Rush.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>