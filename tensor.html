

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Tensor Variables &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="_static/thebelab.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Broadcasting" href="broadcasting.html" />
    <link rel="prev" title="Operations" href="tensorops.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="module2.html">Module 2 - Tensors</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tensordata.html">Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorops.html">Operations</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tensor Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="broadcasting.html">Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="module2.html#tasks">Tasks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Module 4 - Networks</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="module2.html">Module 2 - Tensors</a> &raquo;</li>
        
      <li>Tensor Variables</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/tensor.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tensor-variables">
<h1>Tensor Variables<a class="headerlink" href="#tensor-variables" title="Permalink to this headline">Â¶</a></h1>
<p>Next, we consider autodifferentiation in the tensor framework. We have
now moved from scalars and derivatives to vectors, matrices, and
tensors.  This means <cite>multivariate calculus</cite> can bring into play
somewhat scary terminology. However, most of what we actually does not
require complicated terminology or much technical math. In
fact, except some name changes, we have already built almost
everything we need in <a class="reference internal" href="module1.html"><span class="doc">Module 1 - Auto-Differentiation</span></a>.</p>
<p>The key idea is, just as we had <cite>Scalar</cite> and <cite>ScalarFunction</cite>, we need to
construct <cite>Tensor</cite> and <cite>TensorFunction</cite> (which we just call <cite>Function</cite>).
These new objects behave very similar to their counterparts:</p>
<p>a) Tensors cannot be operated on directly, but need to be transformed through
a Function.
b) Functions must implement both <cite>forward</cite> and <cite>backward</cite>.
c) These transformations are tracked, which allow backpropagation through
the chain rule.</p>
<p>All of this machinery should work out of the box.</p>
<p>The main new terminology to know is <cite>gradient</cite>. Just as a tensor is a
multidimensional array of scalars, a gradient is a multidimensional
array of derivatives for these scalars. Consider the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assignment 1 notation</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">a</span><span class="o">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">derivative</span>


<span class="c1"># Assignment 2 notation</span>
<span class="n">tensor1</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">tensor1</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># shape (3,)</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
<p>The gradient of <cite>tensor1</cite> is a tensor that holds the derivatives of
each of its elements. Another place that gradients come into play is
that <cite>backward</cite> no longer takes <span class="math notranslate nohighlight">\(d_{out}\)</span> as an argument, but now
takes <span class="math notranslate nohighlight">\(grad_{out}\)</span> which is just a tensor consisting of all the
<span class="math notranslate nohighlight">\(d_{out}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will find lots of different notation for
gradients and multivariate terminology. For this Module, you are
supposed to ignore it and stick to everything you know about derivatives.
It turns out that you can do most of machine learning without ever
thinking in higher dimensions.</p>
</div>
<p>If you think about gradient and <span class="math notranslate nohighlight">\(grad_{out}\)</span> in this way
(i.e. tensors of derivatives and <span class="math notranslate nohighlight">\(d_{out}\)</span>),
then you can see how we can easily compute the gradient for tensor
operations using univariate rules.</p>
<ol class="arabic simple">
<li><p><strong>map</strong>. Given a tensor, <cite>map</cite> applies a univariate operation to each scalar
position individually. For a scalar <span class="math notranslate nohighlight">\(x\)</span>, consider computing
<span class="math notranslate nohighlight">\(g(x)\)</span>.  From Module 1, we know that the
derivative of <span class="math notranslate nohighlight">\(f(g(x))\)</span> is equal to <span class="math notranslate nohighlight">\(g'(x) \times d_{out}\)</span>.
To compute the gradient in <cite>backward</cite>, we only need to compute the
derivative for each scalar position and then apply a <cite>mul</cite> map.</p></li>
</ol>
<img alt="_images/map back.png" class="align-center" src="_images/map back.png" />
<ol class="arabic simple" start="2">
<li><p><strong>zip</strong>. Given two tensors, <cite>zip</cite> applies a binary operation to each
pair of scalars. For two scalars <span class="math notranslate nohighlight">\(x\)</span> and
<span class="math notranslate nohighlight">\(y\)</span>, consider computing <span class="math notranslate nohighlight">\(g(x, y)\)</span>.  From Module
1, we know that the derivative of <span class="math notranslate nohighlight">\(f(g(x, y))\)</span> is equal to
<span class="math notranslate nohighlight">\(g_x'(x, y) \times d_{out}\)</span> and <span class="math notranslate nohighlight">\(g_y'(x, y) \times d_{out}\)</span>.
Thus to compute the gradient, we only need to compute the
derivative for each scalar position and apply a <cite>mul</cite> map.</p></li>
</ol>
<img alt="_images/zip back.png" class="align-center" src="_images/zip back.png" />
<ol class="arabic simple" start="3">
<li><p><strong>reduce</strong>. Given a tensor, <cite>reduce</cite> applies an aggregation
operation to one dimension. For simplicity, let's consider sum-based
reductions.  For scalars <span class="math notranslate nohighlight">\(x_1\)</span> to <span class="math notranslate nohighlight">\(x_n\)</span>, consider computing
<span class="math notranslate nohighlight">\(x_1 + x_2 + \ldots + x_n\)</span>.  For any <span class="math notranslate nohighlight">\(x_i\)</span> value, the derivative
is 1.
Therefore, the derivative for any position computed in <cite>backward</cite> is simply
<span class="math notranslate nohighlight">\(d_{out}\)</span>. This means to compute the
gradient, we only need to send <span class="math notranslate nohighlight">\(d_{out}\)</span> to each
position. (For other reduce operations such as <cite>product</cite>, you get
different expansions, which can be calculated just by taking
derivatives).</p></li>
</ol>
<img alt="_images/reduce back.png" class="align-center" src="_images/reduce back.png" />
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="broadcasting.html" class="btn btn-neutral float-right" title="Broadcasting" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="tensorops.html" class="btn btn-neutral float-left" title="Operations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Sasha Rush.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>