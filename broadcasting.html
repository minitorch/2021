

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Broadcasting &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="_static/thebelab.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Module 3 - Efficiency" href="module3.html" />
    <link rel="prev" title="Tensor Variables" href="tensor.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="module2.html">Module 2 - Tensors</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="tensordata.html">Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorops.html">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html">Tensor Variables</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Broadcasting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="module2.html#tasks">Tasks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Module 4 - Networks</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="module2.html">Module 2 - Tensors</a> &raquo;</li>
        
      <li>Broadcasting</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/broadcasting.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="broadcasting">
<h1>Broadcasting<a class="headerlink" href="#broadcasting" title="Permalink to this headline">Â¶</a></h1>
<p>Broadcasting makes tensors convenient and
efficient to use, which comes in handy particularly for <cite>zip</cite> operations.
So far all of our <cite>zip</cite> operations assume two input
tensors of <strong>exactly</strong> the same size and shape. However there are many
interesting cases to <cite>zip</cite> two tensors of different size.</p>
<p>Perhaps the simplest case is we have a vector of size 3 and want to add a
scalar constant to every position:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># In math notation, vector1 + 10</span>
<span class="n">vector1</span> <span class="o">+</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<p>Intuitively, we would like to interpret this expression as the standard
vector+scalar:
adding 10 to each position.
However, the above operation will fail because of shape mistach: we are
adding a tensor of shape(1,) to <cite>vector1</cite> which has shape (3,).</p>
<p>We could ask users to create a tensor of the same shape instead, but it is both
annoying and, more importantly, inefficient:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vector1</span> <span class="o">+</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<p><cite>Broadcasting</cite> is a protocol that allows us to automatically interpret the
frist expression as implying
the second one. Inside <cite>zip</cite>, we pretend that 10 is a vector of
shape (3,) when zipping it with a vector of shape (3,).
Again, this is just an interpretation: we never actually create this vector.</p>
<p>This gives us the first rule of broadcasting:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Rule 1</strong>: Any dimension of size 1 can be zipped with dimensions of size
n &gt; 1 by assuming the dimension  is copied n times.</p>
</div>
<p>Now let's apply this approach to a matrix of shape (4, 3):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">matrix1</span> <span class="o">+</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<p>Here we are trying to zip a matrix (2-D) of shape (4, 3) with a vector
(1-D) of shape (1,). Here we are not just off on the shape, but also on
the number of dimensions.</p>
<p>However, recall that adding an extra dimension of shape-1 doesn't change
the size of the tensor. Therefore we can allow our protocol to add
these in.  Here if we add an empty dimension and then apply rule 1
twice, we can interpret the above expression as an efficient version of:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">matrix1</span> <span class="o">+</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">]</span> <span class="o">*</span> <span class="mi">12</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Rule 2</strong>: Extra dimensions of shape 1 can be added to a tensor to
ensure the same number of dimensions with another tensor.</p>
</div>
<p>Finally, there is a question of where to add the empty dimension. This
is not an issue in the above example but could become an issue in more
complicated cases. Thus we introduce another rule:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Rule 3</strong>: Any extra dimension of size 1 can only be implicitly added
on the left side of the shape.</p>
</div>
<p>This rule has the impact of making the process easy to follow and
replicate. You always know what the shape of the final
output will be. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This will fail: mismatch of (4, 3) and (4,)</span>
<span class="n">matrix1</span> <span class="o">+</span> <span class="n">vector2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="c1"># These two expression are equivalent</span>
<span class="n">matrix1</span> <span class="o">+</span> <span class="n">vector1</span>
<span class="n">matrix1</span> <span class="o">+</span> <span class="n">vector1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>We can apply broadcasting as many times as we want:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The result has shape (3, 2)</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">tensor2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/zip broad.png" src="_images/zip broad.png" />
<img alt="_images/zip broad back.png" src="_images/zip broad back.png" />
<p>Here is a more complicated example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The result has shape (7, 2, 3, 5)</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">tensor2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>We end this guide with two important notes:</p>
<ol class="arabic simple">
<li><p>Broadcasting is <strong>only</strong> about shapes. It does not take strides into
account in any way. It is purely a high-level protocol.</p></li>
<li><p>Broadcasting has some impact on the <cite>backward</cite> pass. We will discuss some
in the code base, but it is not required for any of the tasks.</p></li>
</ol>
<div class="section" id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">Â¶</a></h2>
<ul class="simple">
<li><p>Tensor-Scalar operations can be easily written using broadcasting for
tensors of any dimension.</p></li>
</ul>
<img alt="_images/scalar.png" class="align-center" src="_images/scalar.png" />
<ul class="simple">
<li><p>Matrix-vector operations can be written using broadcasting, but you need
to be careful to make sure that the vector is shaped such the the dimensions
align. This can be done with <cite>view</cite> calls.</p></li>
</ul>
<img alt="_images/vector.png" class="align-center" src="_images/vector.png" />
<ul class="simple">
<li><p>Matrix-matrix operations can be written using broadcasting even when the
dimensions don't align. Here is an example of that process.</p></li>
</ul>
<img alt="_images/threed.png" class="align-center" src="_images/threed.png" />
<ul class="simple">
<li><p>Matrix multiplication can be written in this style, here is <span class="math notranslate nohighlight">\((B x
A^T)\)</span> where A is 3 x 2 and B is 2 x 2 . . (And you will need to use this for
the assignment). However, note this is a memory inefficient way to do matrix
multiplication, as it needs to create an intermediate tensor in the process.</p></li>
</ul>
<img alt="_images/matmul.png" class="align-center" src="_images/matmul.png" />
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="module3.html" class="btn btn-neutral float-right" title="Module 3 - Efficiency" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="tensor.html" class="btn btn-neutral float-left" title="Tensor Variables" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Sasha Rush.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>