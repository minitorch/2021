

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Module 3 - Efficiency &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="_static/thebelab.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Parallel Computation" href="parallel.html" />
    <link rel="prev" title="Broadcasting" href="broadcasting.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="module2.html">Module 2 - Tensors</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Module 3 - Efficiency</a><ul>
<li class="toctree-l2"><a class="reference internal" href="parallel.html">Parallel Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="matrixmult.html">Fusing Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="cuda.html">GPU Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tasks">Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#task-3-1-parallelization">Task 3.1: Parallelization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-3-2-matrix-multiplication">Task 3.2: Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-3-3-cuda-operations">Task 3.3: CUDA Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-3-4-cuda-matrix-multiplication">Task 3.4: CUDA Matrix Multiplication</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-3-4b-extra-credit">Task 3.4b: Extra Credit</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-3-5-training">Task 3.5: Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Module 4 - Networks</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Module 3 - Efficiency</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/module3.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-3-efficiency">
<h1>Module 3 - Efficiency<a class="headerlink" href="#module-3-efficiency" title="Permalink to this headline">¶</a></h1>
<img alt="_images/threadid&#64;3x.png" class="align-center" src="_images/threadid&#64;3x.png" />
<p>In addition to helping simplify code, tensors provide a basis for
speeding up computation. In fact, they are really the only way to
efficiently write deep learning code in a slow language like Python.
However, nothing we have done so far really makes anything faster than
<a class="reference internal" href="module0.html"><span class="doc">Module 0 - Fundamentals</span></a>. This module is focused on taking advantage of tensors
to write fast code, first on standard CPUs and then using GPUs.</p>
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-3">https://github.com/minitorch/Module-3</a> .</p>
<p>To begin, remember to activate your virtual environment first, and then
clone your assignment:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT3_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT_NAME</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Ue</span> <span class="o">.</span>
</pre></div>
</div>
<p>You need the files from previous assignments, so maker sure to pull them over
to your new repo.</p>
<p>Be sure to continue to follow the <a class="reference internal" href="contributing.html"><span class="doc">Contributing</span></a> guidelines.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="parallel.html">Parallel Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrixmult.html">Fusing Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">GPU Programming</a></li>
</ul>
</div>
<div class="section" id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">¶</a></h2>
<p>For this assignment you will need to run you commands in the Google Colab virtual environment.
Follow these instructions for
<a class="reference external" href="https://colab.research.google.com/drive/1zVoQCaTlTU6bkuqMXQTakJxr7oFYdSV2?usp=sharing">Colab setup</a>.</p>
<div class="section" id="task-3-1-parallelization">
<h3>Task 3.1: Parallelization<a class="headerlink" href="#task-3-1-parallelization" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with Numba <cite>prange</cite>.
Be sure to very carefully read the section on
<a class="reference internal" href="parallel.html"><span class="doc">Parallel Computation</span></a>,
<a class="reference external" href="https://numba.pydata.org/numba-doc/latest/user/parallel.html">Numba</a>
and review <a class="reference internal" href="module2.html"><span class="doc">Module 2 - Tensors</span></a>.</p>
</div>
<p>The main backend for our codebase are the three functions <cite>map</cite>,
<cite>zip</cite>, and <cite>reduce</cite>. If we can speed up these three, everything we
built so far will get better. This exercise asks you to utilize Numba
and the <cite>njit</cite> function to speed up these functions. In particular if
you can utilize parallelization through <cite>prange</cite> you can get some big
wins. Be careful though! Parallelization can lead to funny bugs.</p>
<p>In order to help debug this code, we have created a parallel analytics
script for you</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">project</span><span class="o">/</span><span class="n">parallel_test</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Running this script will run NUMBA diagnostics on your functions.</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Complete the following in <cite>minitorch/fast_ops.py</cite> and pass tests
marked as <cite>task3_1</cite>. Furthermore include the diagnostics output
from the above script in your README. Your code should have at
least one parallelized loop.</p>
</div>
<dl class="py function">
<dt id="minitorch.fast_ops.tensor_map">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.fast_ops.tensor_map" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA low_level tensor map function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings floats-to-floats to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for out tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for out tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for out tensor.</p></li>
<li><p><strong>in_storage</strong> (<em>array</em>) -- storage for in tensor.</p></li>
<li><p><strong>in_shape</strong> (<em>array</em>) -- shape for in tensor.</p></li>
<li><p><strong>in_strides</strong> (<em>array</em>) -- strides for in tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.fast_ops.tensor_zip">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_zip</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.fast_ops.tensor_zip" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA higher-order tensor zip function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fn_zip</span> <span class="o">=</span> <span class="n">tensor_zip</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">fn_zip</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Fill in the <cite>out</cite> array by applying <cite>fn</cite> to each
value of <cite>a_storage</cite> and <cite>b_storage</cite> assuming <cite>a_shape</cite>
and <cite>b_shape</cite> broadcast to <cite>out_shape</cite>.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function maps two floats to float to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.fast_ops.tensor_reduce">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_reduce</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.fast_ops.tensor_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA higher-order tensor reduce function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- reduction function mapping two floats to float.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>reduce_shape</strong> (<em>array</em>) -- shape of reduction (1 for dimension kept, shape value for dimensions summed out)</p></li>
<li><p><strong>reduce_size</strong> (<em>int</em>) -- size of reduce shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-3-2-matrix-multiplication">
<h3>Task 3.2: Matrix Multiplication<a class="headerlink" href="#task-3-2-matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with matrix multiplication.
Be sure to read the Guide on
<a class="reference internal" href="matrixmult.html"><span class="doc">Fusing Operations</span></a>.</p>
</div>
<p>Matrix multiplication is key to all of the models that we have trained
so far.  In the last module, we computed matrix multiplication using
broadcasting.  In this task, we ask you to implement it directly as a
function. Do your best to make the function efficient, but for now all
that matters is that you correctly produce a multiply function that
passes our tests and has some parallelism.</p>
<p>In order to use this function, you will also need to add a new
<cite>MatMul</cite> Function to <cite>tensor_functions.py</cite>. We have added a version in
the starter code you can copy.  You might also find it useful to add a
slow broadcasted <cite>matrix_multiply</cite> to <cite>tensor_ops.py</cite> for debugging.</p>
<p>In order to help debug this code, we have created a parallel analytics
script for you</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">project</span><span class="o">/</span><span class="n">parallel_test</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Running this script will run NUMBA diagnostics on your functions.</p>
<p>After you finish this task, you may want to skip to 3.5 and experiment
with training on the real task under speed conditions.</p>
<div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/fast_ops.py</cite> and copy the <cite>MatMul</cite>
Function to <cite>tensor_functions.py</cite>.
Pass tests marked as <cite>task3_2</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.fast_ops.tensor_matrix_multiply">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_matrix_multiply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out</span></em>, <em class="sig-param"><span class="n">out_shape</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">a_storage</span></em>, <em class="sig-param"><span class="n">a_shape</span></em>, <em class="sig-param"><span class="n">a_strides</span></em>, <em class="sig-param"><span class="n">b_storage</span></em>, <em class="sig-param"><span class="n">b_shape</span></em>, <em class="sig-param"><span class="n">b_strides</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.fast_ops.tensor_matrix_multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA tensor matrix multiply function.</p>
<p>Should work for any tensor shapes that broadcast as long as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">a_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-3-3-cuda-operations">
<h3>Task 3.3: CUDA Operations<a class="headerlink" href="#task-3-3-cuda-operations" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with CUDA.
Be sure to read the Guide on
<a class="reference internal" href="cuda.html"><span class="doc">GPU Programming</span></a> and the Numba CUDA guide.</p>
</div>
<p>We can do even better than parallelization if we have access to
specialized hardware. This task asks you to build a GPU implementation
of the backend operations. It will be hard to equal what PyTorch does, but
if you are clever you can make these computations really fast (aim for 2x
of task 3.1).</p>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/cuda_ops.py</cite>, and pass the tests marked as
<cite>task3_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_map">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.cuda_ops.tensor_map" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA higher-order tensor map function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fn_map</span> <span class="o">=</span> <span class="n">tensor_map</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">fn_map</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">...</span> <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings floats-to-floats to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for out tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for out tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for out tensor.</p></li>
<li><p><strong>out_size</strong> (<em>array</em>) -- size for out tensor.</p></li>
<li><p><strong>in_storage</strong> (<em>array</em>) -- storage for in tensor.</p></li>
<li><p><strong>in_shape</strong> (<em>array</em>) -- shape for in tensor.</p></li>
<li><p><strong>in_strides</strong> (<em>array</em>) -- strides for in tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_zip">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_zip</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.cuda_ops.tensor_zip" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA higher-order tensor zipWith (or map2) function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fn_zip</span> <span class="o">=</span> <span class="n">tensor_zip</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">fn_zip</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings two floats to float to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>array</em>) -- size for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_reduce">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_reduce</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.cuda_ops.tensor_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA higher-order tensor reduce function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- reduction function maps two floats to float.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>array</em>) -- size for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>reduce_dim</strong> (<em>int</em>) -- dimension to reduce out</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-3-4-cuda-matrix-multiplication">
<h3>Task 3.4: CUDA Matrix Multiplication<a class="headerlink" href="#task-3-4-cuda-matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with CUDA.
Be sure to read the Guide on
<a class="reference internal" href="cuda.html"><span class="doc">GPU Programming</span></a> and the Numba CUDA guide.</p>
</div>
<p>Finally we can combine both these approaches and implement CUDA
<cite>matmul</cite>. This operation is probably the most important in all of deep
learning and is central to making models fast. Again, we first strive for
accuracy, but, the faster you can make it, the better.</p>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Implement  <cite>minitorch/cuda_ops.py</cite> with CUDA, and pass tests marked as <cite>task3_4</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_matrix_multiply">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_matrix_multiply</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.cuda_ops.tensor_matrix_multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA Kernel object. When called, the kernel object will specialize itself
for the given arguments (if no suitable specialized version already exists)
&amp; compute capability, and launch on the device associated with the current
context.</p>
<p>Kernel objects are not to be constructed by the user, but instead are
created using the <code class="xref py py-func docutils literal notranslate"><span class="pre">numba.cuda.jit()</span></code> decorator.</p>
</dd></dl>

</div>
<div class="section" id="task-3-4b-extra-credit">
<h3>Task 3.4b: Extra Credit<a class="headerlink" href="#task-3-4b-extra-credit" title="Permalink to this headline">¶</a></h3>
<p>Implementing matrix multiplication and reduction efficiently is
hugely important for many deep learning tasks. We have seen one method
for implementing these functions, but there are many more optimizations
that you could apply.</p>
<p>For extra credit, first read these two tutorials:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://numba.pydata.org/numba-doc/latest/cuda/reduction.html">https://numba.pydata.org/numba-doc/latest/cuda/reduction.html</a></p></li>
<li><p><a class="reference external" href="https://nyu-cds.github.io/python-numba/05-cuda/">https://nyu-cds.github.io/python-numba/05-cuda/</a></p></li>
</ul>
<p>Then implement a version of CUDA <cite>tensor_reduce</cite> or
<cite>tensor_matrix_multiply</cite> that take advantage of other aspects of the
GPU.</p>
<p>To get full credit, you should document your code to show us that you
understand each line. Prove to us that these lead to speed-ups on
large matrix operations by making a graph comparing them to naive
operations.</p>
</div>
<div class="section" id="task-3-5-training">
<h3>Task 3.5: Training<a class="headerlink" href="#task-3-5-training" title="Permalink to this headline">¶</a></h3>
<p>If your code works, you should now be able to move on to the tensor
training script in <cite>project/run_fast_tensor.py</cite>.  This code is the same
basic training setup as  <a class="reference internal" href="module2.html"><span class="doc">Module 2 - Tensors</span></a>, but now utilizes your fast tensor
code. We have left the <cite>matmul</cite> layer blank for you to implement with
your tensor code.</p>
<div class="admonition-todo admonition" id="id5">
<p class="admonition-title">Todo</p>
<ul>
<li><p>Implement the missing functions in <cite>project/run_fast_tensor.py</cite>. These
should
do exactly the same thing as the corresponding functions in
<cite>project/run_tensor.py</cite>,
but now use the faster backend</p></li>
<li><p>Train a tensor model and add your results for all dataset
to the README.</p></li>
<li><p>Run a bigger model and record the time per epoch reported by the
trainer. Here is the command</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run_fast_tensor</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">BACKEND</span> <span class="n">gpu</span> <span class="o">--</span><span class="n">HIDDEN</span> <span class="mi">100</span> <span class="o">--</span><span class="n">DATASET</span> <span class="n">split</span> <span class="o">--</span><span class="n">RATE</span> <span class="mf">0.05</span>
<span class="n">python</span> <span class="n">run_fast_tensor</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">BACKEND</span> <span class="n">cpu</span> <span class="o">--</span><span class="n">HIDDEN</span> <span class="mi">100</span> <span class="o">--</span><span class="n">DATASET</span> <span class="n">split</span> <span class="o">--</span><span class="n">RATE</span> <span class="mf">0.05</span>
</pre></div>
</div>
</li>
</ul>
<p>Train a tensor model and add your results for all three dataset
to the README. Also record the time per epoch reported by the
trainer. (As a reference, our parallel implementation gave a 10x speedup).</p>
<blockquote>
<div><p>On a standard Colab GPU setup, aim for you CPU to get below 2 seconds per epoch and
GPU to be below 1 second per epoch. (With some cleverness you can do much better.)</p>
</div></blockquote>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="parallel.html" class="btn btn-neutral float-right" title="Parallel Computation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="broadcasting.html" class="btn btn-neutral float-left" title="Broadcasting" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Sasha Rush.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>