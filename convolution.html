

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Convolution &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="_static/thebelab.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pooling" href="pooling.html" />
    <link rel="prev" title="Module 4 - Networks" href="module4.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="module2.html">Module 2 - Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="module4.html">Module 4 - Networks</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Convolution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#d-convolution">1D Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">2D Convolution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pooling.html">Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="module4.html#tasks">Tasks</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="module4.html">Module 4 - Networks</a> &raquo;</li>
        
      <li>Convolution</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/convolution.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="convolution">
<h1>Convolution<a class="headerlink" href="#convolution" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Convolutions are extensively covered by many excellent
tutorials. In addition to this Guide, we recommend you
to check other tutorials on convolution for more details.</p>
</div>
<p>So far, our main approach to classification problems is to first feed
the input to a <cite>Linear</cite> layer which applies many different linear
seperators to the input, and then apply the ReLU function in order to
transform it into a new hidden representation.</p>
<p>One major problem with the above approach is that it is based on the
<cite>absolute</cite> position of the original input features, which prevents us
from using the same learned parameters on different parts of the
input. Instead, we could have a sliding window (i.e. convolution) that
uses the same set of learned parameters on different local regions of
the input, as shown below:</p>
<img alt="_images/conv.png" class="align-center" src="_images/conv.png" />
<p>Instead of directly transforming the entire input image, the same
convolution is applied at each part of the image to produce a new
representation. It is <cite>sliding</cite> in the sense that, concenptually, the
window over what region to process slides across the entire image to
produce the output.</p>
<p>We will primarily use the convolution on images to understand how to
learn these locally applied parameters, but to get an intuitivie sense
of how convolution works, we will begin with input sequences in 1D.</p>
<div class="section" id="d-convolution">
<h2>1D Convolution<a class="headerlink" href="#d-convolution" title="Permalink to this headline">¶</a></h2>
<p>Our simple 1D convolution takes an input vector of length T and a
weight (or kernel) vector of length K to produce an output vector of
length T.  It computes the output by sliding the weight along the
input, zipping the weight with part of the input, reducing the zipped
result to one value, and then saving this value in the output:</p>
<img alt="_images/conv1d.png" class="align-center" src="_images/conv1d.png" />
<p>If the sliding window goes over the edge of the input, to make things
simpler, we just assume the out-of-edge input values are 0.</p>
<p>An alternative way to think about a convolution is <cite>unrolling</cite> the
input.  Now imagine we have a function named <cite>unroll</cite> which could take
an input tensor and produce a new output tensor:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">unroll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
<p>We then apply matrix multiplication to take the dot-product:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">weight</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
</pre></div>
</div>
<p>We can treat convolution as a fusion of the following two separate
operations (note that we do not implement it this way in practice!):</p>
<img alt="_images/convvec.png" class="align-center" src="_images/convvec.png" />
<p>Given an input and weight, it efficiently unrolls the input and takes
matrix multiplication with weight. This technique is very useful
because it allows you to <cite>hover</cite> over a local segment of the input
sentence. You can think of the weight as capturing a pattern (or many
different patterns) in the original input.</p>
<p>Same as every other operation in the model, we need to be able to
compute the <cite>backward</cite> operation of this 1D convolution. Note that we
can reason through the flow of each cell with matrix
multiplication. Going back to the previous example, the third cell in
the original input (i.e. input[2]) is only utilized to compute three
different cells in the ouput: output[0], output[1] and output[2]</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> \
            <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> \
            <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">output</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> \
            <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<p>Therefore, the gradient calculation is</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grad_input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">grad_output</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">grad_output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">grad_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Visually, it implies that the <cite>backward</cite> of convolution is a
convolution anchored in the opposite side with the reversed weights:</p>
<img alt="_images/conv1dback.png" class="align-center" src="_images/conv1dback.png" />
<p>Similar gradient calculation can be derived for the weight:</p>
<img alt="_images/conv1dback2.png" class="align-center" src="_images/conv1dback2.png" />
<p>The above implies that, same as matrix multiplication, implementing
a fast convolution can be used for both <cite>forward</cite> and <cite>backward</cite>.</p>
<p>Finally, the above approach can be scaled up to handle multiple input
features and multiple weights simultaneously through
<cite>channels</cite>. Analogously to matrix multiplication, we take an
(in_channels, T) input and an (out_channels, in_channels, K) weight to
produce an (out_channels, T). Below is an example with in_channels =
2, out_channels = 3, K = 3 and T = 8.</p>
<a class="reference internal image-reference" href="_images/channels.png"><img alt="_images/channels.png" class="align-center" src="_images/channels.png" style="width: 300px;" /></a>
<p>Codewise (might be a bit harder to
read so we recommend just sticking with the above diagram):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">unroll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Shape: in_channels x K x T</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span> <span class="o">@</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p>1D convolution has all sorts of neat applications: it can be applied
in NLP as a way of applying a model to multiple neighboring words; it
can be used in speech recognition as a way of recognizing important
sounds; it can be used in anomoly detection to find patterns that
trigger an alertl; anywhere you can imagine that a mini-neural network
applying to subset of a sequence can be useful.</p>
</div>
<div class="section" id="id1">
<h2>2D Convolution<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>While 1D convolutions detect patterns along a sequence, 2D
convolutions detect patterns within a grid. The underlying math for
the simple 2D is very similar to the 1D case.  Our simple 2D
convolution takes in an (H, W) input (i.e. height and width) and a
(KH, KW) weight to produce an (H, W) output. The operation is nearly
identical: we walk through each possible unrolled rectangle in the
input matrix, and multiply with the weight. Assuming we had an
analogous unroll function for matrices, this would be equivalent to
computing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">unrolled_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">)</span> <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>
</div>
<p>Another way to think about it is just applying weight as a <cite>Linear</cite>
layer to each one of the rectangles in the input image.</p>
<p>Critically, just as the 1D convolution is anchored at the left,
the 2D convolution is anchored at the top left.
To compute its <cite>backward</cite>, we compute a bottom-right reverse convolution:</p>
<img alt="_images/backward.png" class="align-center" src="_images/backward.png" />
<p>Finally, we can again complicate things by applying many weights to
many input features simultaneously, which gives us the standard 2D
convolution used in Torch.  We take an (in_channels, H, W) input, and
an (out_channels, in_channels, KW, KH) weight matrix to produce an
(out_channels, H, W) output:</p>
<img alt="_images/conv2.png" class="align-center" src="_images/conv2.png" />
<p>Very roughly, the output takes this form:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">unrolled_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">*</span> <span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">)</span> \
         <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
</pre></div>
</div>
<p>2D convolution is the main operator for image recognition systems. It
allows us to process images into local feature representations. It is
also the key step in the convolutional neural network pipeline. It
transforms the input image into hidden features which get propagated
through each stage of the network:</p>
<img alt="_images/networkcnn.png" class="align-center" src="_images/networkcnn.png" />
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="pooling.html" class="btn btn-neutral float-right" title="Pooling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="module4.html" class="btn btn-neutral float-left" title="Module 4 - Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Sasha Rush.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>